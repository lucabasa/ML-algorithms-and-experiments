{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "\n",
    "Following [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), it performs a regression trying to minimize the sum of squared error (as the linear regression above does) with a penalty on the size of the coefficients. Therefore, it tries to minimize the following quantity\n",
    "$$ ||y - Xw||^2_2 + \\alpha  ||w||^2_2 , $$ \n",
    "where $w$ is the coefficient vector and the parameter $\\alpha$ controls the amount of regularization (called $L_2$ as it uses the $L_2$ norm of the coefficient vector). The higher this parameter, the lower will be the variance of the model.\n",
    "\n",
    "It is a very useful regularization in case of multicollinearity in the input data that tipically lead to very large coefficients. The regularization is putting a penalty on larger coefficients, thus reducing the effect of multicollinearity.\n",
    "\n",
    "On the contrary of the simple linear regression, we have some hyperparameters to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import source.hyperplots as hyp\n",
    "import source.explore as exp\n",
    "from source.report import plot_predictions, make_results, store_results\n",
    "from source.utility import cv_score, grid_search\n",
    "import source.transf_univ as df_p\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge, Perceptron\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* alpha: the regularization and most important hyperparameter to tune. It must be positive. When set to 0, we get the simple Linear regression.\n",
    "* copy_X: that does not affect the results of the model but, if False, may lead to overwriting the input data\n",
    "* fit_intercept: whether or not using an intercept term, not relevant for our cases as we, rightfully, always scale and center our data\n",
    "* max_iter: a parameter that control the maximum number of iterations the solver can take to converge. Very rarely it needs to be increased and, if so, we might need to consider a different approach\n",
    "* normalize: whether or not normalizing the data and it is ignored if set to False. If True, it uses the StandardScaler we already have in our pipeline\n",
    "* random_state: relevant only for one of the solver since it is stochastic (the 'sag' solver)\n",
    "* solver: the type of solver. Almost always leaving to `auto` is a good move as it will pick the best one for the data provided. The coiche will influence the speed and the behavior in particular situations.\n",
    "* tol: the precision of the solution\n",
    "\n",
    "## Noise and correlation\n",
    "\n",
    "We can now repeat the previous experiments and observe how the algorithm is behaving differently even in the simplest case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_ridge(data_name, kfolds, store=False, sample=False, **kwargs):\n",
    "    df = pd.read_csv(data_name)\n",
    "    if sample:\n",
    "        df = df.sample(sample)\n",
    "    \n",
    "    coefs_name = data_name.split('.csv')[0] + '__coefficients.csv'\n",
    "    target_name = data_name.split('/')[2].split('.csv')[0]\n",
    "\n",
    "    target = df['target']\n",
    "\n",
    "    df_train = df.drop('target', axis=1)\n",
    "\n",
    "    model = Pipeline([('processing', numeric_pipe),\n",
    "                      ('scl', df_p.df_scaler()), \n",
    "                      ('ridge', Ridge(**kwargs))])\n",
    "\n",
    "    oof, coefs_est = cv_score(df_train, target, kfolds, model, imp_coef=True)\n",
    "\n",
    "    plot_predictions(df_train, target, oof, feature=None, hue=None, legend=False, savename=False)\n",
    "\n",
    "    hyp.plot_coefficients('tar_nonlin_3', coefs_est, \n",
    "                          coefs_real=coefs_name)\n",
    "    \n",
    "    if len(kwargs.keys()) == 0:\n",
    "        kwargs = {'alpha': 1}\n",
    "    \n",
    "    if store:\n",
    "        store_results('data/03_linear_models.csv', \n",
    "                      label=target, prediction=oof, model='Ridge', \n",
    "                      parameters=kwargs, \n",
    "                      target_name=target_name, variables='All', instances=df_train.shape[0], verbose=True)\n",
    "    else:\n",
    "        res = make_results(label=target, prediction=oof, model='Ridge', \n",
    "                           parameters=kwargs, \n",
    "                           target_name=target_name, variables='All', instances=df_train.shape[0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ridge('data/simulated/10_feat_10_inf_nonoise.csv', kfolds, store=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the coefficients are again estimated perfectly, the residuals show a pattern that suggests that the model is having more and more troubles in predicting larger (in the absolute sense) values of the target variable.\n",
    "\n",
    "On the other hand, the behavior in presence of noise and/or correlation is the nearly identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ridge('data/simulated/10_feat_10_inf_noise.csv', kfolds, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ridge('data/simulated/100_feat_65_inf_noise_rank.csv', kfolds, store=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of instances\n",
    "\n",
    "Due to the fact that some of the solvers are faster, the only difference we observe is in the speed. The training time still grows linearly, but with a lower rate of increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_learning_curve(data_name, **kwargs):\n",
    "    df = pd.read_csv(data_name)\n",
    "    \n",
    "    title = data_name.split('/')[2].split('.csv')[0]\n",
    "\n",
    "    target = df['target']\n",
    "\n",
    "    df_train = df.drop('target', axis=1)\n",
    "\n",
    "    model = Pipeline([('processing', numeric_pipe),\n",
    "                      ('scl', df_p.df_scaler()), \n",
    "                      ('ridge', Ridge(**kwargs))])\n",
    "\n",
    "    hyp.plot_learning_curve(model, title, \n",
    "                            df_train, target, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_learning_curve('data/simulated/10_feat_10_inf_noise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_learning_curve('data/simulated/100_feat_65_inf_noise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ridge('data/simulated/10_feat_10_inf_nonoise.csv', kfolds, store=True, sample=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first time we notice that the regularized model is performing better than the simple linear regression in presence of noise when the we have a limited number of instances. The better performance is visible in all the metrics and in the coefficients estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ridge('data/simulated/10_feat_10_inf_noise.csv', kfolds, store=True, sample=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "All the above experiments were performed using the default hyperparameters, which generally gives a good indication of the model performance to compare results. However, this time we can control more the behavior of the model. This section will do just that.\n",
    "\n",
    "First, we need to generate the results for different model configurations. We do so by using the custom grid search function imported from the utility module.\n",
    "\n",
    "We will explore configurations that differ per regularization, precision, and solver type. Due to the need of seeing some pattern, we will use a dataset without noise in order to have errors at an order of magnitude that let us see the variation when the hyperparameter varies. Later, using more complex dataset, we will see more interesting patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('processing', numeric_pipe),\n",
    "                  ('scl', df_p.df_scaler()),\n",
    "                  ('ridge', Ridge(random_state=235))])\n",
    "\n",
    "param_grid = {'ridge__alpha': list(np.arange(0.05, 10, 0.05)), \n",
    "              'ridge__tol': [0.00001, 0.001, 0.1, 1], \n",
    "              'ridge__solver': ['svd', 'sparse_cg', 'lsqr', 'saga']}\n",
    "\n",
    "df = pd.read_csv('data/simulated/100_feat_65_inf_nonoise.csv').sample(300)\n",
    "\n",
    "target = df['target']\n",
    "\n",
    "df_train = df.drop('target', axis=1)\n",
    "\n",
    "res, bp, _ = grid_search(df_train, target, model, param_grid, 'neg_mean_squared_error', kfolds)\n",
    "\n",
    "print(bp)\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we see how the choice of the solver is influencing the training time, with `lsqr` and `sparse_cg` being the fastest. On the other hand, different solvers seem to get different scores but it is worth noticing that, by increasing the regularization, this difference becomes less and less relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp.plot_hyperparameter(res[(res.param_ridge__alpha == 0.5) & (res.param_ridge__tol==0.001)], 'param_ridge__solver', 'Solver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp.plot_hyperparameter(res[(res.param_ridge__alpha == 5) & (res.param_ridge__tol==0.001)], 'param_ridge__solver', 'Solver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an artifact of the role of alpha in the model performance, which in this case (a purely linear regression) is getting worse and worse when alpha gets bigger. At the same time, we also notice that this choice does not influence the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp.plot_hyperparameter(res[(res.param_ridge__solver=='lsqr') & (res.param_ridge__tol==0.001)], 'param_ridge__alpha', 'Alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp.plot_two_hyperparms(res[(res.param_ridge__solver=='lsqr')], 'param_ridge__alpha', 'param_ridge__tol', 'Alpha vs Tolerance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
